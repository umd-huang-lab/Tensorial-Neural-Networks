% Part 1: (General) Tensor decompositions

@inproceedings{cohen2016convolutional,
	title={Convolutional rectifier networks as generalized tensor decompositions},
	author={Cohen, Nadav and Shashua, Amnon},
	booktitle={International Conference on Machine Learning},
	pages={955--963},
	year={2016}
}

@inproceedings{khrulkov2018expressive,	
	title={Expressive power of recurrent neural networks},
	author={Khrulkov, Valentin and Novikov, Alexander and Oseledets, Ivan},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{wang2017tensor,
	title={Tensor decomposition via simultaneous power iteration},
	author={Wang, Po-An and Lu, Chi-Jen},
	booktitle={International Conference on Machine Learning},
	pages={3665--3673},
	year={2017}
}

@article{comon2009tensor,
	title={Tensor decompositions, alternating least squares and other tales},
	author={Comon, Pierre and Luciani, Xavier and De Almeida, Andr{\'e} LF},
	journal={Journal of Chemometrics: A Journal of the Chemometrics Society},
	volume={23},
	number={7-8},
	pages={393--405},
	year={2009},
	publisher={Wiley Online Library}
}

% CP & Tucker Decomposition:
@article{kolda2009tensor,
	title={Tensor decompositions and applications},
	author={Kolda, Tamara G and Bader, Brett W},
	journal={SIAM review},
	volume={51},
	number={3},
	pages={455--500},
	year={2009},
	publisher={SIAM}
}

% Tucker Decomposition
@article{tucker1966some,
    title={Some mathematical notes on three-mode factor analysis},
    author={Tucker, Ledyard R},
    journal={Psychometrika},
    volume={31},
    number={3},
    pages={279--311},
    year={1966},
    publisher={Springer}
}

% Tensor-Train Decomposition:
@article{oseledets2011tensor,
	title={Tensor-train decomposition},
	author={Oseledets, Ivan V},
	journal={SIAM Journal on Scientific Computing},
	volume={33},
	number={5},
	pages={2295--2317},
	year={2011},
	publisher={SIAM}
}

% Tensor-Ring Decomposition
@article{zhao2016tensor,
	title={Tensor ring decomposition},
	author={Zhao, Qibin and Zhou, Guoxu and Xie, Shengli and Zhang, Liqing and Cichocki, Andrzej},
	journal={arXiv preprint arXiv:1606.05535},
	year={2016}
}

% Tensor Network
@article{grasedyck2013literature,
	title={A literature survey of low-rank tensor approximation techniques},
	author={Grasedyck, Lars and Kressner, Daniel and Tobler, Christine},
	journal={GAMM-Mitteilungen},
	volume={36},
	number={1},
	pages={53--78},
	year={2013},
	publisher={Wiley Online Library}
}

% Tensor Network
@article{orus2014practical,
	title={A practical introduction to tensor networks: Matrix product states and projected entangled pair states},
	author={Or{\'u}s, Rom{\'a}n},
	journal={Annals of Physics},
	volume={349},
	pages={117--158},
	year={2014},
	publisher={Elsevier}
}

% Tensor Network
@article{cichocki2016tensor,
	title={Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 1 Low-Rank Tensor Decompositions},
	author={Cichocki, Andrzej and Lee, Namgil and Oseledets, Ivan and Phan, Anh-Huy and Zhao, Qibin and Mandic, Danilo P and others},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
	volume={9},
	number={4-5},
	pages={249--429},
	year={2016},
	publisher={Now Publishers, Inc.}
}

% Tensor Network
@article{cichocki2017tensor,
	title={Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives},
	author={Cichocki, Andrzej and Phan, Anh-Huy and Zhao, Qibin and Lee, Namgil and Oseledets, Ivan and Sugiyama, Masashi and Mandic, Danilo P and others},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
  	volume={9},
  	number={6},
  	pages={431--673},
  	year={2017},
  	publisher={Now Publishers, Inc.}
}

% Tensorial Neural Networks
@article{su2018tensorial,
    title={Tensorial neural networks: Generalization of neural networks and application to model compression},
    author={Su, Jiahao and Li, Jingling and Bhattacharjee, Bobby and Huang, Furong},
    journal={arXiv preprint arXiv:1805.10352},
    year={2018}
}

% Convolutional Tensor-Train Decomposition
@article{su2020convolutional,
	title={Convolutional Tensor-Train LSTM for Spatio-temporal Learning},
	author={Su, Jiahao and Byeon, Wonmin and Huang, Furong and Kautz, Jan and Anandkumar, Animashree},
	journal={arXiv preprint arXiv:2002.09131},
	year={2020}
}

% Part 2: (Deep) Neural Networks

% AlexNet
@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}

% VGG
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

% Interleaved group convolutions
@inproceedings{zhang2017interleaved,
	title={Interleaved group convolutions},
	author={Zhang, Ting and Qi, Guo-Jun and Xiao, Bin and Wang, Jingdong},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={4373--4382},
	year={2017}
}

% Xception: depth-wise separable convolutions
@inproceedings{chollet2017xception,
    title={Xception: Deep learning with depthwise separable convolutions},
    author={Chollet, Fran{\c{c}}ois},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={1251--1258},
    year={2017}
}

% Inception
@inproceedings{szegedy2015going,
    title={Going deeper with convolutions},
    author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={1--9},
    year={2015}
}

% Inception
@inproceedings{szegedy2017inception,
	title={Inception-v4, inception-resnet and the impact of residual connections on learning.},
	author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
	booktitle={AAAI},
	volume={4},
	pages={12},
	year={2017}
}

% NIN
@article{lin2013network,
	title={Network in network},
	author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
	journal={arXiv preprint arXiv:1312.4400},
	year={2013}
}




% ResNet
@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

% ResNet - v2
@inproceedings{he2016identity,
	title={Identity mappings in deep residual networks},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={European Conference on Computer Vision},
  	pages={630--645},
	year={2016},
	organization={Springer}
}

% Wide ResNet (WRN)
@inproceedings{zagoruyko2016wide,
    title={Wide Residual Networks},
    author={Zagoruyko, Sergey and Komodakis, Nikos},
    booktitle={British Machine Vision Conference 2016},
  year={2016},
  organization={British Machine Vision Association}
}

% DenseNet
@inproceedings{huang2017densely,
	title={Densely connected convolutional networks.},
	author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	booktitle={CVPR},
	volume={1},
	pages={3},
	year={2017}
}

% MobileNet v1: depth-wise separable convolutions
@article{howard2017mobilenets,
    title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
    author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
    journal={arXiv preprint arXiv:1704.04861},
    year={2017}
}

% MobileNet v2:
@inproceedings{sandler2018mobilenetv2,
    title={Mobilenetv2: Inverted residuals and linear bottlenecks},
    author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={4510--4520},
    year={2018}
}

% MobileNet v3:
@inproceedings{howard2019searching,
    title={Searching for MobileNetV3},
    author={Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and others},
    booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
    pages={1314--1324},
    organization={IEEE}
}

% ShuffleNet v1:
@inproceedings{zhang2018shufflenet,
    title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
    author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={6848--6856},
    year={2018}
}

% ShuffleNet v2:
@inproceedings{ma2018shufflenet,
    title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
    author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
    booktitle={Proceedings of the European conference on computer vision (ECCV)},
    pages={116--131},
    year={2018}
}

% ein-conv
@article{hayashi2019exploring,
    title={Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks},
    author={Hayashi, Kohei and Yamaguchi, Taiki and Sugawara, Yohei and Maeda, Shin-ichi},
    journal={Advances in Neural Information Processing Systems},
    volume={32},
    pages={5552--5562},
    year={2019}
}

% Part 3: Neural network compression techniques

% Surveys of neural network compression
@article{cheng2017survey,
	title={A survey of model compression and acceleration for deep neural networks},
	author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
	journal={arXiv preprint arXiv:1710.09282},
	year={2017}
}

@article{deng2020model,
    title={Model compression and hardware acceleration for neural networks: A comprehensive survey},
    author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
    journal={Proceedings of the IEEE},
    volume={108},
    number={4},
    pages={485--532},
    year={2020},
    publisher={IEEE}
}

% Dictionary Learning:
@inproceedings{zhang2015efficient,
	title={Efficient and accurate approximations of nonlinear convolutional networks},
  	author={Zhang, Xiangyu and Zou, Jianhua and Ming, Xiang and He, Kaiming and Sun, Jian},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={1984--1992},
	year={2015}
}

% Scheme 1: Dictionary Learning + Separable filter / Scheme 2: SVD:
@inproceedings{jaderberg2014speeding,
    title={Speeding up Convolutional Neural Networks with Low Rank Expansions},
    author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
    booktitle={Proceedings of the British Machine Vision Conference. BMVA Press},
    year={2014}
}

% CP decomposition and unbalanced SVD
@inproceedings{denton2014exploiting,
	title={Exploiting linear structure within convolutional networks for efficient evaluation},
	author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	booktitle={Advances in neural information processing systems},
	pages={1269--1277},
	year={2014}
}

% CP decomposition:
@inproceedings{lebedev2015speeding,
    title={Speeding-up convolutional neural networks using fine-tuned CP-decomposition},
    author={Lebedev, V and Ganin, Y and Rakhuba, M and Oseledets, I and Lempitsky, V},
    booktitle={3rd International Conference on Learning Representations, ICLR 2015-Conference Track Proceedings},
    year={2015}
}

% Tucker decomposition:
@article{kim2015compression,
	title={Compression of deep convolutional neural networks for fast and low power mobile applications},
	author={Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
	journal={arXiv preprint arXiv:1511.06530},
	year={2015}
}

% Train-train decomposition:
@article{garipov2016ultimate,
	title={Ultimate tensorization: compressing convolutional and FC layers alike},
	author={Garipov, Timur and Podoprikhin, Dmitry and Novikov, Alexander and Vetrov, Dmitry},
	journal={arXiv preprint arXiv:1611.03214},
	year={2016}
}

% Tensor-ring decomposition
@inproceedings{wang2018wide,
  title={Wide compression: Tensor ring nets},
  author={Wang, Wenqi and Sun, Yifan and Eriksson, Brian and Wang, Wenlin and Aggarwal, Vaneet},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9329--9338},
  year={2018}
}

% CP decomposition (Tensor contraction layer)
@inproceedings{kossaifi2017tensor,
	title={Tensor contraction layers for parsimonious deep nets},
	author={Kossaifi, Jean and Khanna, Aran and Lipton, Zachary and Furlanello, Tommaso and Anandkumar, Anima},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
	pages={26--32},
	year={2017}
}

% Tucker decomposition (Tensor regression layer)
@article{kossaifi2020tensor,
	title={Tensor regression networks},
	author={Kossaifi, Jean and Lipton, Zachary C and Kolbeinsson, Arinbjorn and Khanna, Aran and Furlanello, Tommaso and Anandkumar, Anima},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={123},
	pages={1--21},
	year={2020}
}

% Tensor-Train Decomposition:
@inproceedings{novikov2015tensorizing,
	title={Tensorizing neural networks},
	author={Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
	booktitle={Advances in Neural Information Processing Systems},
	pages={442--450},
	year={2015}
}

% Block-term Decomposition
@article{ye2020block,
    title={Block-term tensor neural networks},
    author={Ye, Jinmian and Li, Guangxi and Chen, Di and Yang, Haiqin and Zhe, Shandian and Xu, Zenglin},
    journal={Neural Networks},
    year={2020},
    publisher={Elsevier}
}

% Recurrent neural network (RNN, LSTM and GRU)
@inproceedings{yang2017tensor,
    title={Tensor-train recurrent neural networks for video classification},
    author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
    booktitle={International Conference on Machine Learning},
    pages={3891--3900},
    year={2017},
    organization={PMLR}
}

% Polynomial Tensor-train recurrent neural network
@article{yu2017long,
	title={Long-term forecasting using tensor-train RNNs},
	author={Yu, Rose and Zheng, Stephan and Anandkumar, Anima and Yue, Yisong},
	journal={arXiv preprint arXiv:1711.00073},
	year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

% knowledge distillation
@article{hinton2015distilling,
    title={Distilling the knowledge in a neural network},
    author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
    journal={arXiv preprint arXiv:1503.02531},
    year={2015}
}

@article{ba2014deep,
    title={Do Deep Nets Really Need to be Deep?},
    author={Ba, Jimmy and Caruana, Rich},
    journal={Advances in Neural Information Processing Systems},
    volume={27},
    year={2014}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{hubara2016binarized,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={4114--4122},
  year={2016}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@article{li2018learning,
  title={Learning IoT in edge: Deep learning for the Internet of Things with edge computing},
  author={Li, He and Ota, Kaoru and Dong, Mianxiong},
  journal={IEEE network},
  volume={32},
  number={1},
  pages={96--101},
  year={2018},
  publisher={IEEE}
}

@inproceedings{li2020understanding,
  title={Understanding generalization in deep learning via tensor methods},
  author={Li, Jingling and Sun, Yanchao and Su, Jiahao and Suzuki, Taiji and Huang, Furong},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={504--515},
  year={2020},
  organization={PMLR}
}

@article{pfeifer2014faster,
  title={Faster identification of optimal contraction sequences for tensor networks},
  author={Pfeifer, Robert NC and Haegeman, Jutho and Verstraete, Frank},
  journal={Physical Review E},
  volume={90},
  number={3},
  pages={033315},
  year={2014},
  publisher={APS}
}

@article{chi1997optimizing,
  title={On optimizing a class of multi-dimensional loops with reduction for parallel execution},
  author={Chi-Chung, Lam and Sadayappan, P and Wenger, Rephael},
  journal={Parallel Processing Letters},
  volume={7},
  number={02},
  pages={157--168},
  year={1997},
  publisher={World Scientific}
}


% Tensor Decomposition on ResNet
@inproceedings{reustle2020fast,
  title={Fast GPU Convolution for CP-Decomposed Tensorial Neural Networks},
  author={Reustle, Alexander and Rabbani, Tahseen and Huang, Furong},
  booktitle={Proceedings of SAI Intelligent Systems Conference},
  pages={468--487},
  year={2020},
  organization={Springer}
}

@article{daniel2018opt,
  title={Opt$\backslash$\_einsum-a python package for optimizing contraction order for einsum-like expressions},
  author={Daniel, G and Gray, Johnnie and others},
  journal={Journal of Open Source Software},
  volume={3},
  number={26},
  pages={753},
  year={2018}
}

@article{Soomro2012UCF101AD,
  title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},
  author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  journal={ArXiv},
  year={2012},
  volume={abs/1212.0402}
}

@inproceedings{simonyan2014two,
  title={Two-Stream Convolutional Networks for Action Recognition in Videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={Advances in Neural Information Processing Systems},
  pages={568--576},
  year={2014}
}

@misc{huang2019twostream,
  author = {Huang, Yi and Shrivastava, Rajat},
  title = {Two-Stream Action Recognition},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/jeffreyyihuang/two-stream-action-recognition},
  commit = {8458458b4abe97e36c4987148a7dfcb24273cafe}
}

@misc{einops,
  author = {Rogozhnikov, Alex and Garcia, Cristian},
  title = {Einops},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/arogozhnikov/einops}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

% CIFAR-10/100 dataset
@article{krizhevsky2009learning,
    title={Learning Multiple Layers of Features from Tiny Images},
    author={Krizhevsky, A},
    journal={Master's thesis, University of Tront},
    year={2009}
}

@article{khan2020survey,
  title={A survey of the recent architectures of deep convolutional neural networks},
  author={Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
  journal={Artificial Intelligence Review},
  volume={53},
  number={8},
  pages={5455--5516},
  year={2020},
  publisher={Springer}
}

% ImageNet dataset
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

% Numpy
@article{harris2020array,
  title={Array programming with NumPy},
  author={Harris, Charles R and Millman, K Jarrod and van der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group}
}

% Pytorch
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

% TensorLy
@article{kossaifi2019tensorly,
  title={TensorLy: Tensor Learning in Python},
  author={Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={26},
  pages={1--6},
  year={2019}
}

% Tensor Comprehension
@article{vasilache2018tensor,
  title={Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions},
  author={Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  journal={arXiv preprint arXiv:1802.04730},
  year={2018}
}

@article{lee2021qttnet,
  title={QTTNet: Quantized tensor train neural networks for 3D object and video recognition},
  author={Lee, Donghyun and Wang, Dingheng and Yang, Yukuan and Deng, Lei and Zhao, Guangshe and Li, Guoqi},
  journal={Neural Networks},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{goyal2019compression,
  title={Compression of Deep Neural Networks by combining pruning and low rank decomposition},
  author={Goyal, Saurabh and Choudhury, Anamitra Roy and Sharma, Vivek},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={952--958},
  year={2019},
  organization={IEEE}
}