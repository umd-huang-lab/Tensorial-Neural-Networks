\section{Conclusions and Discussions}
\label{sec:conclusion}

In this paper, we introduce an open-source library \autotnn, which is capable of building and efficiently training TNNs. Our \autotnn is competitive against and in many cases, superior to \pytorch TNN training.
For future work, we plan to further accelerate training and test times through incorporation of parallel computation paradigms into the intra-layer tensor computations. {Since our \autotnn relies on the \pytorch backend, we can also use tensorRT to further accelerate OpenTNN. Additionally, we will investigate incorporating quantization, pruning, and knowledge distillation techniques directly into \autotnn.}